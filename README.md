# ConnectFour - AlphaZero

An implementation of AlphaZero in TensorFlow. 
Self-play data generation was distributed on a linux cluster using the ray framework.
Training was done on Google Colab.

## Self-play

We will create a folder structure like below:

```
.
|-- model
|   |-- gen-0
|   |   |-- frozen_model.pb
|   |   |-- keras_model.h5
|   |   |-- tf
|   |   |   |-- checkpoint
|   |   |   |-- connect4.data-00000-of-00001
|   |   |   |-- connect4.index
|   |   |   `-- connect4.meta
|   |   `-- weights.h5
|   |
|   `-- gen-1
|       |-- frozen_model.pb
|       |-- keras_model.h5
|       |-- tf
|       |   |-- checkpoint
|       |   |-- connect4.data-00000-of-00001
|       |   |-- connect4.index
|       |   `-- connect4.meta
|       `-- weights.h5
|
`-- training
    |-- gen-0
    |   `-- c7b6e919-819e-4881-834c-12e9ae058d49
    |       |-- 0_5_self_play.pkl
    |       |-- 1_5_self_play.pkl
    |       |-- 2_5_self_play.pkl
    |       `-- selfplay.log
    |
    `-- gen-1
        |-- 9c396ef0-b2d3-4f84-bffb-05732f0802c2
        |   |-- 0_5_self_play.pkl
        |   |-- 1_5_self_play.pkl
        |   |-- 2_5_self_play.pkl
        |   `-- selfplay.log
        |
        `-- feb9a4aa-9a9e-4418-8759-3708dd6548a6
            |-- 0_64_self_play.pkl
            |-- 1_64_self_play.pkl
            `-- selfplay.log
            
```

* model: contains the trained models
* training: contains the self-play data, we will use this to create new generations

### Initialising the 1st generation

To create the first generation, we can use the "init_generation_zero" method.
We can directly access it by running the script from the command line.

```bash
python utils.py -o /example/path
```

This will create a random model inside the folder "/example/path/model/gen-0".
      
### Creating 2nd, 3rd, etc.. 
To speed up the self-play data generation, we can use any cluster of linux machines.

Create the head node:

```bash
ray start --head --redis-port=12345
```    

And for all of the other nodes:

```bash
ray start --address=10.1.2.3:12345
```

To start the self-play process, run the generate.py script on the head node (or any other node).
Number of worker processes should be the number of cpu cores in total.

```bash
python generate.py -g gen-0 -b 32 -n 100 -w 64 -r 10.1.2.3:12345
```    

Every worker will have its own uuid and will create a folder under the currently used generation, 
so they can be used on a shared filesystem.
(See folder structure above.)

If we don't want to use a cluster, we can simply start the script without the -r flag 
(and fewer workers)
```bash
python generate.py -g gen-0 -b 32 -n 100 -w 1
```    


## Training
We need to use the self-play, generated by the previous models.

```bash
python train.py -pg gen-6 -ng gen-7 -b 32 -n 10
```
This will create a new, trained model under the new folder "gen-7". 
We will not use all of the previously created data, 
but slowly leave out the early self-plays (of the weaker players).



## Evaluation

Evaluation is only used for creating statistics.
This is not necessary for the training, we can let self-play / training cycle run continuously.

gen-0 vs gen-1

```bash
python evaluate.py -pg gen-0 -ng gen-1 -o /example/path
```

First 100 games will be played with new-gen as first player, 
then another 100 games with new-gen as second player.

The results will be saved under /example/path/evaluations.log 

